

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lingvo.core.attention module &mdash; lingvo  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="lingvo.core.attention_test module" href="lingvo.core.attention_test.html" />
    <link rel="prev" title="lingvo.core.ops.tokenizer_ops_test module" href="lingvo.core.ops.tokenizer_ops_test.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="lingvo.html">lingvo package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="lingvo.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="lingvo.core.html">lingvo.core package</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="lingvo.core.html#subpackages">Subpackages</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="lingvo.core.html#submodules">Submodules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tasks.html">lingvo.tasks package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tools.html">lingvo.tools package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lingvo.html#submodules">Submodules</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="lingvo.html">lingvo package</a> &raquo;</li>
        
          <li><a href="lingvo.core.html">lingvo.core package</a> &raquo;</li>
        
      <li>lingvo.core.attention module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/lingvo.core.attention.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-lingvo.core.attention">
<span id="lingvo-core-attention-module"></span><h1>lingvo.core.attention module<a class="headerlink" href="#module-lingvo.core.attention" title="Permalink to this headline">¶</a></h1>
<p>Attention models.</p>
<dl class="class">
<dt id="lingvo.core.attention.AdditiveAttention">
<em class="property">class </em><code class="descclassname">lingvo.core.attention.</code><code class="descname">AdditiveAttention</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#AdditiveAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.AdditiveAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.attention.BaseAttentionLayer" title="lingvo.core.attention.BaseAttentionLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.attention.BaseAttentionLayer</span></code></a></p>
<p>Implements additive attention (also known as “Bahdanau Attention”),
as described in:</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.
“Neural Machine Translation by Jointly Learning to Align and Translate.”
ICLR 2015.
<a class="reference external" href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a></p>
<dl class="method">
<dt id="lingvo.core.attention.AdditiveAttention.ComputeContextVectorWithSource">
<code class="descname">ComputeContextVectorWithSource</code><span class="sig-paren">(</span><em>theta</em>, <em>concated_source_vecs</em>, <em>concated_source_contexts</em>, <em>source_padding</em>, <em>source_segment_id</em>, <em>query_vec</em>, <em>attention_state=None</em>, <em>per_step_source_padding=None</em>, <em>step_state=None</em>, <em>query_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#AdditiveAttention.ComputeContextVectorWithSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.AdditiveAttention.ComputeContextVectorWithSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the context vector given the current query output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>concated_source_vecs</strong> – Concated source vectors with shape [time,
batch_size, hidden_dim].</li>
<li><strong>concated_source_contexts</strong> – Concated source contexts with shape [
batch_size, time, context_dim].</li>
<li><strong>source_padding</strong> – Source padding with shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – Tensor of source segment ids, with shape [time,
batch_size].</li>
<li><strong>query_vec</strong> – a tensor of shape [batch_size, query_dim].</li>
<li><strong>attention_state</strong> – previous attention state. It is not used in
AdditiveAttention, and is simply passed through.</li>
<li><strong>per_step_source_padding</strong> – Source sequence padding to apply at this step.
If not None, it should be of shape [target_batch_size,
source_seq_length].</li>
<li><strong>step_state</strong> – A NestedMap containing ‘global_step’ and ‘time_step’.
Required for deterministic dropout.</li>
<li><strong>query_segment_id</strong> – a tensor of shape [batch_size]</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Note: concated_source_vecs are the vectors that are used to compute the
attention score between the query_vec and each concated_source_vec.
The concated_source_contexts are the vectors that compose the result.
The attention context vector is computed as a weighted average of the
concated_source_contexts, using the scores that were computed using
concated_source_vecs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">[batch_size, context_dim]
The attention probability vector: [batch_size, time]
The new attention mechanism state: possibly nested tuple of tensors with<blockquote>
<div>dimensions [target_batch….]</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">The attention context vector</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.AdditiveAttention.InitForSourcePacked">
<code class="descname">InitForSourcePacked</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#AdditiveAttention.InitForSourcePacked"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.AdditiveAttention.InitForSourcePacked" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize attention for the given source vectors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A single tensor of shape [time, batch_size, source_dim].</li>
<li><strong>source_contexts</strong> – A single tensor of shape [time, batch_size, some_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, batch_size].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Concated source vectors, concated source contexts, and source paddings.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.AdditiveAttention.PackSource">
<code class="descname">PackSource</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#AdditiveAttention.PackSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.AdditiveAttention.PackSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs source vectors. Does not change attention state.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A single tensor of shape [time, batch_size, source_dim].</li>
<li><strong>source_contexts</strong> – A single tensor of shape [time, batch_size, some_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, batch_size].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Concated source vectors, concated source contexts, and source paddings.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="lingvo.core.attention.AdditiveAttention.Params">
<em class="property">classmethod </em><code class="descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#AdditiveAttention.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.AdditiveAttention.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for this AdditiveAttention class.</p>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.AdditiveAttention.ZeroAttentionState">
<code class="descname">ZeroAttentionState</code><span class="sig-paren">(</span><em>source_seq_length</em>, <em>decoder_batch_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#AdditiveAttention.ZeroAttentionState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.AdditiveAttention.ZeroAttentionState" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="lingvo.core.attention.BaseAttentionLayer">
<em class="property">class </em><code class="descclassname">lingvo.core.attention.</code><code class="descname">BaseAttentionLayer</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#BaseAttentionLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.BaseAttentionLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.LayerBase" title="lingvo.core.base_layer.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.LayerBase</span></code></a></p>
<p>A base class for all attention layers.</p>
<dl class="method">
<dt id="lingvo.core.attention.BaseAttentionLayer.ComputeContextVector">
<code class="descname">ComputeContextVector</code><span class="sig-paren">(</span><em>theta</em>, <em>query_vec</em>, <em>attention_state=None</em>, <em>per_step_source_padding=None</em>, <em>step_state=None</em>, <em>query_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#BaseAttentionLayer.ComputeContextVector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.BaseAttentionLayer.ComputeContextVector" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the context vector given the current query output.</p>
<p>Unlike ComputeContextVectorWithSource, which explicitly asks for the source
tensors (concated_source_vecs, concated_source_contexts, source_padding),
ComputeContextVector uses the class’ internal variables.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>query_vec</strong> – a tensor of shape [batch_size, query_dim].</li>
<li><strong>attention_state</strong> – previous attention state.</li>
<li><strong>per_step_source_padding</strong> – Source sequence padding to apply at this step.
If not None, it should be of shape [target_batch_size,
source_seq_length].</li>
<li><strong>step_state</strong> – A NestedMap containing ‘global_step’ and ‘time_step’.
Required for deterministic dropout.</li>
<li><strong>query_segment_id</strong> – a tensor of shape [batch_size].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><p>The attention context vector.
The attention probability vector.
The new attention mechanism state: possibly nested tuple of tensors with</p>
<blockquote>
<div><p>dimensions [target_batch….]</p>
</div></blockquote>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.BaseAttentionLayer.ComputeContextVectorWithSource">
<code class="descname">ComputeContextVectorWithSource</code><span class="sig-paren">(</span><em>theta</em>, <em>concated_source_vecs</em>, <em>concated_source_contexts</em>, <em>source_padding</em>, <em>source_segment_id</em>, <em>query_vec</em>, <em>attention_state=None</em>, <em>per_step_source_padding=None</em>, <em>step_state=None</em>, <em>query_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#BaseAttentionLayer.ComputeContextVectorWithSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.BaseAttentionLayer.ComputeContextVectorWithSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the context vector given the current query output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>concated_source_vecs</strong> – Concated source vectors with shape [time,
batch_size, hidden_dim].</li>
<li><strong>concated_source_contexts</strong> – Concated source contexts with shape [
batch_size, time, context_dim].</li>
<li><strong>source_padding</strong> – Source padding with shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – Source segment ids with shape [time, batch_size].</li>
<li><strong>query_vec</strong> – a tensor of shape [batch_size, query_dim].</li>
<li><strong>attention_state</strong> – previous attention state.</li>
<li><strong>per_step_source_padding</strong> – Source sequence padding to apply at this step.
If not None, it should be of shape [target_batch_size,
source_seq_length].</li>
<li><strong>step_state</strong> – A NestedMap containing ‘global_step’ and ‘time_step’.
Required for deterministic dropout.</li>
<li><strong>query_segment_id</strong> – a tensor of shape [batch_size].</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Note: concated_source_vecs are the vectors that are used to compute the
attention score between the query_vec and each concated_source_vec.
The concated_source_contexts are the vectors that compose the result.
The attention context vector is computed as a weighted average of the
concated_source_contexts, using the scores that were computed using
concated_source_vecs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">[batch_size, context_dim]
The attention probability vector: [batch_size, time]
The new attention mechanism state: possibly nested tuple of tensors with<blockquote>
<div>dimensions [target_batch….]</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">The attention context vector</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.BaseAttentionLayer.GetInitializationSourceState">
<code class="descname">GetInitializationSourceState</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#BaseAttentionLayer.GetInitializationSourceState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.BaseAttentionLayer.GetInitializationSourceState" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the attention initialization state.</p>
<p>The base class only preserves the concated_source_vecs,
concated_source_contexts and source_padding. If subclasses use more
state than this and need to interact with inference code that must
fetch and reload state, this and SetInitializationSourceState must
be overridden.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A NestedMap of Tensors that can be preserved and reset via
SetInitializationSourceState() at a later point. This allows, for example,
for attention computations to span session runs.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.BaseAttentionLayer.InitForSourcePacked">
<code class="descname">InitForSourcePacked</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#BaseAttentionLayer.InitForSourcePacked"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.BaseAttentionLayer.InitForSourcePacked" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize attention for the given source vectors.</p>
<p>Must set _source_init_done to True in the function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A single tensor of shape [time, batch_size, source_dim].</li>
<li><strong>source_contexts</strong> – A single tensor of shape [time, batch_size, some_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, batch_size].
source_segment_id is not None for packed inputs where one training
example may pack multiple sequences.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Note: source_segment_id, if present, should always have the same shape as
source_padding.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><dl class="docutils">
<dt>A tuple (concated_source_vecs, concated_source_contexts, source_padding,</dt>
<dd>source_segment_id),</dd>
</dl>
<p>where concated_source_vecs is a tensor of shape [time, batch_size,
hidden_dim], concated_source_contexts is a tensor of shape [batch_size,
time, some_dim], source_padding is a tensor of shape [time,
batch_size], source_segment_id is a tensor of shape [time, batch_size].
Note the mismatch between concated_source_vecs and
concated_source_contexts. In concated_source_vecs, time is the first dim,
while it is the second dim in concated_source_contexts.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="lingvo.core.attention.BaseAttentionLayer.Params">
<em class="property">classmethod </em><code class="descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#BaseAttentionLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.BaseAttentionLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.BaseAttentionLayer.SetInitializationSourceState">
<code class="descname">SetInitializationSourceState</code><span class="sig-paren">(</span><em>new_init_state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#BaseAttentionLayer.SetInitializationSourceState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.BaseAttentionLayer.SetInitializationSourceState" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the attention initialization state.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>new_init_state</strong> – A NestedMap matching what was returned from</li>
<li><strong>which will return this layer to that</strong> (<em>GetInitializationSourceState</em><em>,</em>) – </li>
<li><strong>state.</strong> (<em>initialization</em>) – </li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="lingvo.core.attention.DotProductAttention">
<em class="property">class </em><code class="descclassname">lingvo.core.attention.</code><code class="descname">DotProductAttention</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#DotProductAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.DotProductAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.attention.BaseAttentionLayer" title="lingvo.core.attention.BaseAttentionLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.attention.BaseAttentionLayer</span></code></a></p>
<p>Implements dot-product attention (also known as “Luong Attention”)
as described in:</p>
<p>Minh-Thang Luong, Hieu Pham, Christopher D. Manning.
“Effective Approaches to Attention-based Neural Machine Translation.”
EMNLP 2015.
<a class="reference external" href="https://arxiv.org/abs/1508.04025">https://arxiv.org/abs/1508.04025</a></p>
<dl class="method">
<dt id="lingvo.core.attention.DotProductAttention.ComputeContextVectorWithSource">
<code class="descname">ComputeContextVectorWithSource</code><span class="sig-paren">(</span><em>theta</em>, <em>concated_source_vecs</em>, <em>concated_source_contexts</em>, <em>source_padding</em>, <em>source_segment_id</em>, <em>query_vec</em>, <em>attention_state=None</em>, <em>per_step_source_padding=None</em>, <em>step_state=None</em>, <em>query_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#DotProductAttention.ComputeContextVectorWithSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.DotProductAttention.ComputeContextVectorWithSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the context vector given the current query output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>concated_source_vecs</strong> – Concated source vectors with shape [time,
source_batch, hidden_dim].</li>
<li><strong>concated_source_contexts</strong> – Concated source contexts with shape [
source_batch, time, context_dim].</li>
<li><strong>source_padding</strong> – Source padding with shape [time, source_batch].</li>
<li><strong>source_segment_id</strong> – Source segment id with shape [time, source_batch].</li>
<li><strong>query_vec</strong> – a tensor of shape [target_batch, query_dim], where
target_batch = n * source_batch (e.g., n = num_hyps_per_beam in
beamsearch). Along the target_batch dimension, there are n groups of
consecutive rows, each group containing source_batch rows.</li>
<li><strong>attention_state</strong> – previous attention state. It is not used in
AdditiveAttention, and is simply passed through.</li>
<li><strong>per_step_source_padding</strong> – Source sequence padding to apply at this step.
If not None, it should be of shape [target_batch, source_seq_length].</li>
<li><strong>step_state</strong> – A NestedMap containing ‘global_step’ and ‘time_step’.
Required for deterministic dropout.</li>
<li><strong>query_segment_id</strong> – Query segment id with shape [target_batch].</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Note: concated_source_vecs are the vectors that are used to compute the
attention score between the query_vec and each concated_source_vec.
The concated_source_contexts are the vectors that compose the result.
The attention context vector is computed as a weighted average of the
concated_source_contexts, using the scores that were computed using
concated_source_vecs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">[target_batch, context_dim]
The attention probability vector: [target_batch, time]
The new attention mechanism state: possibly nested tuple of tensors with<blockquote>
<div>dimensions [target_batch….]</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">The attention context vector</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.DotProductAttention.InitForSourcePacked">
<code class="descname">InitForSourcePacked</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#DotProductAttention.InitForSourcePacked"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.DotProductAttention.InitForSourcePacked" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize attention for the given source vectors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A tensor of shape [time, source_batch, source_dim].</li>
<li><strong>source_contexts</strong> – A tensor of shape [time, source_batch, context_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, source_batch].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, source_batch].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tuple (concated_source_vecs, concated_source_contexts, source_padding),
where concated_source_vecs is a tensor of shape [time, batch_size,
hidden_dim], concated_source_contexts is a tensor of shape [batch_size,
time, some_dim] and source_padding is a tensor of shape [time,
batch_size]. Note the mismatch between concated_source_vecs and
concated_source_contexts. In concated_source_vecs, time is the first dim,
while it is the second dim in concated_source_contexts.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.DotProductAttention.PackSource">
<code class="descname">PackSource</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#DotProductAttention.PackSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.DotProductAttention.PackSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs source vectors. Does not change attention state.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A tensor of shape [time, source_batch, source_dim].</li>
<li><strong>source_contexts</strong> – A tensor of shape [time, source_batch, context_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, source_batch].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, source_batch].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tuple (concated_source_vecs, concated_source_contexts, source_padding),
where concated_source_vecs is a tensor of shape [time, batch_size,
hidden_dim], concated_source_contexts is a tensor of shape [batch_size,
time, some_dim] and source_padding is a tensor of shape [time,
batch_size]. Note the mismatch between concated_source_vecs and
concated_source_contexts. In concated_source_vecs, time is the first dim,
while it is the second dim in concated_source_contexts.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="lingvo.core.attention.DotProductAttention.Params">
<em class="property">classmethod </em><code class="descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#DotProductAttention.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.DotProductAttention.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for DotProductAttention.</p>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.DotProductAttention.ZeroAttentionState">
<code class="descname">ZeroAttentionState</code><span class="sig-paren">(</span><em>source_seq_length</em>, <em>decoder_batch_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#DotProductAttention.ZeroAttentionState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.DotProductAttention.ZeroAttentionState" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="lingvo.core.attention.GmmMonotonicAttention">
<em class="property">class </em><code class="descclassname">lingvo.core.attention.</code><code class="descname">GmmMonotonicAttention</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#GmmMonotonicAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.GmmMonotonicAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.attention.BaseAttentionLayer" title="lingvo.core.attention.BaseAttentionLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.attention.BaseAttentionLayer</span></code></a></p>
<p>A GMM-based monotonic attention module.</p>
<p>Based on “Generating Sequences With Recurrent Neural Networks” by Alex Graves.
Eq [46-51] in <a class="reference external" href="https://arxiv.org/abs/1308.0850">https://arxiv.org/abs/1308.0850</a>.</p>
<dl class="method">
<dt id="lingvo.core.attention.GmmMonotonicAttention.ComputeContextVectorWithSource">
<code class="descname">ComputeContextVectorWithSource</code><span class="sig-paren">(</span><em>theta</em>, <em>concated_source_vecs</em>, <em>concated_source_contexts</em>, <em>source_padding</em>, <em>source_segment_id</em>, <em>query_vec</em>, <em>attention_state</em>, <em>per_step_source_padding=None</em>, <em>step_state=None</em>, <em>query_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#GmmMonotonicAttention.ComputeContextVectorWithSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.GmmMonotonicAttention.ComputeContextVectorWithSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the context vector given the current query output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>concated_source_vecs</strong> – Concated source vectors with shape [time,
batch_size, hidden_dim].</li>
<li><strong>concated_source_contexts</strong> – Concated source contexts with shape [
batch_size, time, context_dim].</li>
<li><strong>source_padding</strong> – Source padding with shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – Tensor of source segment ids, with shape [time,
batch_size].</li>
<li><strong>query_vec</strong> – a tensor of shape [batch_size, query_dim].</li>
<li><strong>attention_state</strong> – <p>previous attention state,
then attention_state is a tensor of shape [batch_size, num_mixtures, 4]:</p>
<blockquote>
<div>attention_state[:, :, 0] contains previous location
attention_state[:, :, 1] contains previous offset.
attention_state[:, :, 2] contains previous variance.
attention_state[:, :, 3] contains previous prior.</div></blockquote>
</li>
<li><strong>per_step_source_padding</strong> – Source sequence padding to apply at this step.
If not None, it should be of shape [target_batch_size,
source_seq_length].</li>
<li><strong>step_state</strong> – A NestedMap containing ‘global_step’ and ‘time_step’.
Required for deterministic dropout.</li>
<li><strong>query_segment_id</strong> – a tensor of shape [batch_size]</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Note: concated_source_vecs are the vectors that are used to compute the
attention score between the query_vec and each concated_source_vec.
The concated_source_contexts are the vectors that compose the result.
The attention context vector is computed as a weighted average of the
concated_source_contexts, using the scores that were computed using
concated_source_vecs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">[batch_size, context_dim]
The attention probability vector: [batch_size, time]
The new attention state vector.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">The attention context vector</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.GmmMonotonicAttention.InitForSourcePacked">
<code class="descname">InitForSourcePacked</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#GmmMonotonicAttention.InitForSourcePacked"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.GmmMonotonicAttention.InitForSourcePacked" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize attention for the given source vectors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A single tensor of shape [time, batch_size, source_dim].</li>
<li><strong>source_contexts</strong> – A single tensor of shape [time, batch_size, some_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, batch_size].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Concated source vectors, concated source contexts, source paddings
and source_segment_id.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="lingvo.core.attention.GmmMonotonicAttention.Params">
<em class="property">classmethod </em><code class="descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#GmmMonotonicAttention.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.GmmMonotonicAttention.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for this MonotonicAttention class.</p>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.GmmMonotonicAttention.ZeroAttentionState">
<code class="descname">ZeroAttentionState</code><span class="sig-paren">(</span><em>source_seq_length</em>, <em>decoder_batch_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#GmmMonotonicAttention.ZeroAttentionState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.GmmMonotonicAttention.ZeroAttentionState" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="lingvo.core.attention.LocationSensitiveAttention">
<em class="property">class </em><code class="descclassname">lingvo.core.attention.</code><code class="descname">LocationSensitiveAttention</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#LocationSensitiveAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.LocationSensitiveAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.attention.BaseAttentionLayer" title="lingvo.core.attention.BaseAttentionLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.attention.BaseAttentionLayer</span></code></a></p>
<p>An attention that also takes into account previously attended locations.</p>
<p>See section 2.2 of this paper for a description of this technique:
<a class="reference external" href="http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf">http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf</a></p>
<dl class="method">
<dt id="lingvo.core.attention.LocationSensitiveAttention.ComputeContextVectorWithSource">
<code class="descname">ComputeContextVectorWithSource</code><span class="sig-paren">(</span><em>theta</em>, <em>concated_source_vecs</em>, <em>concated_source_contexts</em>, <em>source_padding</em>, <em>source_segment_id</em>, <em>query_vec</em>, <em>attention_state=None</em>, <em>per_step_source_padding=None</em>, <em>step_state=None</em>, <em>query_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#LocationSensitiveAttention.ComputeContextVectorWithSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.LocationSensitiveAttention.ComputeContextVectorWithSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the context vector given the current query output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>concated_source_vecs</strong> – Concated source vectors with shape [time,
batch_size, hidden_dim].</li>
<li><strong>concated_source_contexts</strong> – Concated source contexts with shape [
batch_size, time, context_dim].</li>
<li><strong>source_padding</strong> – Source padding with shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – Source segment id with shape [time, batch_size].</li>
<li><strong>query_vec</strong> – a tensor of shape [batch_size, query_dim].</li>
<li><strong>attention_state</strong> – <dl class="docutils">
<dt>If params().location_features == [‘PREV_PROBS’,</dt>
<dd>’CUMULATIVE_PROBS’],</dd>
<dt>then attention_state is a tensor of shape [batch_size, src_len * 2]:</dt>
<dd>attention_state[:, :, 0] contains previous attention probabilities
attention_state[:, :, 1] contains a sum over previous timesteps of
attention probabilities.</dd>
</dl>
</li>
<li><strong>per_step_source_padding</strong> – Source sequence padding to apply at this step.
If not None, it should be of shape [target_batch_size,
source_seq_length].</li>
<li><strong>step_state</strong> – A NestedMap containing ‘global_step’ and ‘time_step’.
Required for deterministic dropout.</li>
<li><strong>query_segment_id</strong> – Query segment id with shape [batch_size].</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Note: concated_source_vecs are the vectors that are used to compute the
attention score between the query_vec and each concated_source_vec.
The concated_source_contexts are the vectors that compose the result.
The attention context vector is computed as a weighted average of the
concated_source_contexts, using the scores that were computed using
concated_source_vecs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">[batch_size, context_dim]
The attention probability vector: [batch_size, time]
The new attention mechanism state: possibly nested tuple of tensors with<blockquote>
<div>dimensions [target_batch….]</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">The attention context vector</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.LocationSensitiveAttention.InitForSourcePacked">
<code class="descname">InitForSourcePacked</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#LocationSensitiveAttention.InitForSourcePacked"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.LocationSensitiveAttention.InitForSourcePacked" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize attention for the given source vectors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A single tensor of shape [time, batch_size, source_dim].</li>
<li><strong>source_contexts</strong> – A single tensor of shape [time, batch_size, some_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, batch_size].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Concated source vectors, concated source contexts, and source paddings.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="lingvo.core.attention.LocationSensitiveAttention.Params">
<em class="property">classmethod </em><code class="descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#LocationSensitiveAttention.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.LocationSensitiveAttention.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for this LocationSensitiveAttention class.</p>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.LocationSensitiveAttention.ZeroAttentionState">
<code class="descname">ZeroAttentionState</code><span class="sig-paren">(</span><em>source_seq_length</em>, <em>decoder_batch_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#LocationSensitiveAttention.ZeroAttentionState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.LocationSensitiveAttention.ZeroAttentionState" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="lingvo.core.attention.MergeSourcePaddingWithPerStepSourcePadding">
<code class="descclassname">lingvo.core.attention.</code><code class="descname">MergeSourcePaddingWithPerStepSourcePadding</code><span class="sig-paren">(</span><em>source_padding</em>, <em>per_step_source_padding</em>, <em>tb</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MergeSourcePaddingWithPerStepSourcePadding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MergeSourcePaddingWithPerStepSourcePadding" title="Permalink to this definition">¶</a></dt>
<dd><p>Merges source padding with per-step source padding.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>source_padding</strong> – [sl, sb].</li>
<li><strong>per_step_source_padding</strong> – [tb, sl].</li>
<li><strong>tb</strong> – target batch size.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tensor of shape [tb, sl].</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="lingvo.core.attention.MonotonicAttention">
<em class="property">class </em><code class="descclassname">lingvo.core.attention.</code><code class="descname">MonotonicAttention</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MonotonicAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MonotonicAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.attention.BaseAttentionLayer" title="lingvo.core.attention.BaseAttentionLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.attention.BaseAttentionLayer</span></code></a></p>
<p>An attention mechanism which enforces monotonic alignments.</p>
<p>This layer implements the monotonic attention mechanism described in <a href="#id1"><span class="problematic" id="id2">``</span></a>Online
and Linear-Time Attention by Enforcing Mononotonic Alignments’’
(<a class="reference external" href="https://arxiv.org/abs/1704.00784">https://arxiv.org/abs/1704.00784</a>).  It is used in exactly the same way as
AdditiveAttention, but both the attention distribution and the energy function
are different.
Rather than using a softmax, this mechanism feeds the attention energy into a
(hard or soft) sigmoid and treats the output as Bernoulli probabilities
representing the probability of attending to a given entry in the input
sequence, processed from left-to-right.  Based on this interpretation, the
resulting distribution over input sequence entries is computed with a dynamic
program.  The intended use is to train with soft sigmoids according to the
expected output (setting param hard_sigmoid=False), then use hard sigmoids at
test time to allow for online and linear-time decoding.  To encourge the train
and test-time behavior to be similar, noise can optionally be added to the
sigmoid activations during training (param pre_sigmoid_noise).  For the energy
function, rather than computing
energy = dot(v, tanh(dot(W, query) + dot(W, encoder_states)))
it computes
energy = dot(g*v/||v||, tanh(dot(W, query) + dot(W, encoder_states) + b)) + r
where g and r are scalars and b is a vector, and ||v|| is the L2 norm of v.
instead.  These modifications address the fact that the sigmoids in the
monotonic attention mechanism are sensitive to offset and a bit harder to
train compared to the softmax function.  It can be helpful to initialize the
energy bias scalar r to a negative value (param hidden_bias_init).</p>
<dl class="method">
<dt id="lingvo.core.attention.MonotonicAttention.ComputeContextVectorWithSource">
<code class="descname">ComputeContextVectorWithSource</code><span class="sig-paren">(</span><em>theta</em>, <em>concated_source_vecs</em>, <em>concated_source_contexts</em>, <em>source_padding</em>, <em>source_segment_id</em>, <em>query_vec</em>, <em>attention_state</em>, <em>per_step_source_padding=None</em>, <em>step_state=None</em>, <em>query_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MonotonicAttention.ComputeContextVectorWithSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MonotonicAttention.ComputeContextVectorWithSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the context vector given the current query output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>concated_source_vecs</strong> – Concated source vectors with shape [time,
batch_size, hidden_dim].</li>
<li><strong>concated_source_contexts</strong> – Concated source contexts with shape [
batch_size, time, context_dim].</li>
<li><strong>source_padding</strong> – Source padding with shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – Source segment id with shape [time, batch_size].</li>
<li><strong>query_vec</strong> – a tensor of shape [batch_size, query_dim].</li>
<li><strong>attention_state</strong> – The attention probs computed at the previous timestep.</li>
<li><strong>per_step_source_padding</strong> – Source sequence padding to apply at this step.
If not None, it should be of shape [target_batch_size,
source_seq_length].</li>
<li><strong>step_state</strong> – A NestedMap containing ‘global_step’ and ‘time_step’.
Required for deterministic dropout.</li>
<li><strong>query_segment_id</strong> – a tensor of shape [batch_size].</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Note: concated_source_vecs are the vectors that are used to compute the
attention score between the query_vec and each concated_source_vec.
The concated_source_contexts are the vectors that compose the result.
The attention context vector is computed as a weighted average of the
concated_source_contexts, using the scores that were computed using
concated_source_vecs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">[batch_size, context_dim]
The attention probability vector: [batch_size, time]
The attention probability vector (again, to be interpreted as state).</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">The attention context vector</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MonotonicAttention.ComputeProbabilities">
<code class="descname">ComputeProbabilities</code><span class="sig-paren">(</span><em>theta</em>, <em>concated_source_vecs</em>, <em>merged_source_padding</em>, <em>query_vec</em>, <em>attention_state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MonotonicAttention.ComputeProbabilities"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MonotonicAttention.ComputeProbabilities" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes probabilities of emissions.</p>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MonotonicAttention.InitForSourcePacked">
<code class="descname">InitForSourcePacked</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MonotonicAttention.InitForSourcePacked"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MonotonicAttention.InitForSourcePacked" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize attention for the given source vectors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A single tensor of shape [time, batch_size, source_dim].</li>
<li><strong>source_contexts</strong> – A single tensor of shape [time, batch_size, some_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, batch_size].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Concated source vectors, concated source contexts, and source paddings.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MonotonicAttention.PackSource">
<code class="descname">PackSource</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MonotonicAttention.PackSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MonotonicAttention.PackSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs source vectors. Does not change attention state.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A single tensor of shape [time, batch_size, source_dim].</li>
<li><strong>source_contexts</strong> – A single tensor of shape [time, batch_size, some_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, batch_size].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Concated source vectors, concated source contexts, and source paddings.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="lingvo.core.attention.MonotonicAttention.Params">
<em class="property">classmethod </em><code class="descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MonotonicAttention.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MonotonicAttention.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for this MonotonicAttention class.</p>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MonotonicAttention.PostTrainingStepUpdate">
<code class="descname">PostTrainingStepUpdate</code><span class="sig-paren">(</span><em>global_step</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MonotonicAttention.PostTrainingStepUpdate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MonotonicAttention.PostTrainingStepUpdate" title="Permalink to this definition">¶</a></dt>
<dd><p>Update self._step_counter with the global_step value.</p>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MonotonicAttention.ZeroAttentionState">
<code class="descname">ZeroAttentionState</code><span class="sig-paren">(</span><em>source_seq_length</em>, <em>decoder_batch_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MonotonicAttention.ZeroAttentionState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MonotonicAttention.ZeroAttentionState" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="lingvo.core.attention.MultiHeadedAttention">
<em class="property">class </em><code class="descclassname">lingvo.core.attention.</code><code class="descname">MultiHeadedAttention</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MultiHeadedAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MultiHeadedAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.attention.BaseAttentionLayer" title="lingvo.core.attention.BaseAttentionLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.attention.BaseAttentionLayer</span></code></a></p>
<p>Attention with multiple attention heads.</p>
<dl class="docutils">
<dt>Conceptually, the algorithm works as follows:</dt>
<dd>1. Source vectors (attention keys) are first projected to vectors of dim
p.hidden_dim.
2. Query vectors are projected to vectors of dim p.hidden_dim as well.
3. Context vectors (attention values) are not projected.
4. Source vectors, query vectors and context vectors are all split into
p.num_attention_heads chunks.
5. The inner atten mechanism is computed separately on each of the chunks.
6. Attention contexts from each of the chunk are concatenated to form the
final context.
7. Attention probs from each of the chunk are averaged to form the final
attention prob.</dd>
</dl>
<dl class="method">
<dt id="lingvo.core.attention.MultiHeadedAttention.ComputeContextVectorWithAttenProbs">
<code class="descname">ComputeContextVectorWithAttenProbs</code><span class="sig-paren">(</span><em>theta</em>, <em>packed_context</em>, <em>atten_probs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MultiHeadedAttention.ComputeContextVectorWithAttenProbs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MultiHeadedAttention.ComputeContextVectorWithAttenProbs" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the context vector given the attention probailities.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>packed_context</strong> – Concated source contexts with shape [
batch_size * num_heads, time, context_dim // num_heads].</li>
<li><strong>atten_probs</strong> – The attention probability vector:
[batch_size * num_heads, time].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">[target_batch, source_dim]
If p.enable_ctx_post_proj is false, source_dim = context_dim,
otherwise, source_dim = p.ctx_post_proj_dim.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">The attention context vector</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MultiHeadedAttention.ComputeContextVectorWithCachedSource">
<code class="descname">ComputeContextVectorWithCachedSource</code><span class="sig-paren">(</span><em>theta</em>, <em>concated_source_vecs</em>, <em>concated_source_contexts</em>, <em>source_padding</em>, <em>source_segment_id</em>, <em>query_vec</em>, <em>attention_state=None</em>, <em>per_step_source_padding=None</em>, <em>step_state=None</em>, <em>query_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MultiHeadedAttention.ComputeContextVectorWithCachedSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MultiHeadedAttention.ComputeContextVectorWithCachedSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as the ComputeContextVectorWithSource api above, except values …</p>
<p>in source_vecs, source_contexts and source_padding are ordered differently.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>concated_source_vecs</strong> – Concated source vectors with shape [source_batch,
time, hidden_dim].</li>
<li><strong>concated_source_contexts</strong> – Concated source contexts with shape [
source_batch, time, context_dim].</li>
<li><strong>source_padding</strong> – Source padding with shape [source_batch, time, num_heads].
If None, assume no padding.</li>
<li><strong>source_segment_id</strong> – Source segment id with shape
[source_batch, time, num_heads].</li>
<li><strong>query_vec</strong> – a tensor of shape [target_batch, query_dim].</li>
<li><strong>attention_state</strong> – previous attention state. It is not used in
AdditiveAttention, and is simply passed through.</li>
<li><strong>per_step_source_padding</strong> – Source sequence padding to apply at this step.
If not None, it should be of shape [target_batch_size,
source_seq_length].</li>
<li><strong>step_state</strong> – A NestedMap containing ‘global_step’ and ‘time_step’.
Required for deterministic dropout.</li>
<li><strong>query_segment_id</strong> – a tensor of shape [target_batch].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>[target_batch, source_dim]
The attention probability vector: [target_batch, time]
The new attention mechanism state: possibly nested tuple of tensors with</p>
<blockquote>
<div><p>dimensions [target_batch….]</p>
</div></blockquote>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">The attention context vector</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MultiHeadedAttention.ComputeContextVectorWithSource">
<code class="descname">ComputeContextVectorWithSource</code><span class="sig-paren">(</span><em>theta</em>, <em>concated_source_vecs</em>, <em>concated_source_contexts</em>, <em>source_padding</em>, <em>source_segment_id</em>, <em>query_vec</em>, <em>attention_state=None</em>, <em>per_step_source_padding=None</em>, <em>step_state=None</em>, <em>query_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MultiHeadedAttention.ComputeContextVectorWithSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MultiHeadedAttention.ComputeContextVectorWithSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the context vector given the current query output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>concated_source_vecs</strong> – Concated source vectors with shape [time,
batch_size, hidden_dim].</li>
<li><strong>concated_source_contexts</strong> – Concated source contexts with shape [
batch_size, time, context_dim].</li>
<li><strong>source_padding</strong> – Source padding with shape [time, batch_size].</li>
<li><strong>source_segment_id</strong> – Source segment id with shape [time, batch_size].</li>
<li><strong>query_vec</strong> – a tensor of shape [target_batch, query_dim].</li>
<li><strong>attention_state</strong> – previous attention state. It is not used in
AdditiveAttention, and is simply passed through.</li>
<li><strong>per_step_source_padding</strong> – Source sequence padding to apply at this step.
If not None, it should be of shape [target_batch_size,
source_seq_length].</li>
<li><strong>step_state</strong> – A NestedMap containing ‘global_step’ and ‘time_step’.
Required for deterministic dropout.</li>
<li><strong>query_segment_id</strong> – a tensor of shape [target_batch].</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Note: concated_source_vecs are the vectors that are used to compute the
attention score between the query_vec and each concated_source_vec.
The concated_source_contexts are the vectors that compose the result.
The attention context vector is computed as a weighted average of the
concated_source_contexts, using the scores that were computed using
concated_source_vecs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">[target_batch, context_dim]
The attention probability vector: [target_batch, time]
The new attention mechanism state: possibly nested tuple of tensors with<blockquote>
<div>dimensions [target_batch….]</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">The attention context vector</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MultiHeadedAttention.ExtendSourcePacked">
<code class="descname">ExtendSourcePacked</code><span class="sig-paren">(</span><em>theta</em>, <em>new_source_vecs</em>, <em>new_source_contexts</em>, <em>new_source_paddings</em>, <em>new_source_segment_ids</em>, <em>cached_prev_source_vecs</em>, <em>cached_prev_source_contexts</em>, <em>cached_prev_source_paddings</em>, <em>cached_prev_source_segment_ids</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MultiHeadedAttention.ExtendSourcePacked"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MultiHeadedAttention.ExtendSourcePacked" title="Permalink to this definition">¶</a></dt>
<dd><p>Extend cached source_vecs and source_contexts by one more timestep.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>new_source_vecs</strong> – A tensor of shape [source_batch, source_dim].</li>
<li><strong>new_source_contexts</strong> – A tensor of shape [source_batch, context_dim].
new_source_vecs and new_source_contexts are source_vecs and
source_contexts for the new timestep to be extended.</li>
<li><strong>new_source_paddings</strong> – If not None, a tensor of shape [source_batch].
source_padding for the new timestep.</li>
<li><strong>new_source_segment_ids</strong> – If not None, a tensor of shape [source_batch].
source_segment_id for the new timestep.</li>
<li><strong>cached_prev_source_vecs</strong> – A tensor of shape [source_batch,
t - 1, hidden_dim].</li>
<li><strong>cached_prev_source_contexts</strong> – A tensor of shape [source_batch,
t - 1, hidden_dim].
‘cached_prev_source_vecs’ and ‘cached_prev_source_contexts’ are the
already preprocessed source_vecs and source_contexts for the previous
t-1 steps.</li>
<li><strong>cached_prev_source_paddings</strong> – If not None, a tensor of shape [source_batch,
t - 1, num_heads], cached source padding for the previous t - 1
timesteps.</li>
<li><strong>cached_prev_source_segment_ids</strong> – If not None, a tensor of shape
[source_batch, t - 1, num_heads], cached source segment id for the
previous t - 1 timesteps.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Extended cached source_vecs, source_contexts and source_paddings.
‘extended_source_vec’ is of shape [batch_size, t, num_heads * dim],
‘extended_source_context’ is of shape [batch_size, t, num_heads * dim],
source_padding is of shape [batch_size, t, num_heads], source_segment_id
is of shape [batch_size, t, num_heads].</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MultiHeadedAttention.InitForSourcePacked">
<code class="descname">InitForSourcePacked</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MultiHeadedAttention.InitForSourcePacked"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MultiHeadedAttention.InitForSourcePacked" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize attention for the given source vectors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A tensor of shape [time, source_batch, source_dim].</li>
<li><strong>source_contexts</strong> – A tensor of shape [time, source_batch, context_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, source_batch].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, source_batch].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">(concated_source_vecs, concated_source_contexts, source_padding,
source_segment_id) tuple where concated_source_vecs is a tensor of shape
[source_seq_len, batch_size * num_heads, orig_source_dim / num_heads],
concated_source_contexts is a tensor of shape [source_batch_size *
num_heads, source_seq_len,  orig_context_dim / num_heads],
source_padding is a tensor of shape [source_seq_len, batch_size *
num_heads] and source_segment_id is a tensor of shape
[source_seq_len, batch_size * num_heads].</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MultiHeadedAttention.PackSource">
<code class="descname">PackSource</code><span class="sig-paren">(</span><em>theta</em>, <em>source_vecs</em>, <em>source_contexts</em>, <em>source_padding</em>, <em>source_segment_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MultiHeadedAttention.PackSource"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MultiHeadedAttention.PackSource" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs source vectors. Does not change attention state.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> – A nested map object containing weights’ values of this
layer and its children layers.</li>
<li><strong>source_vecs</strong> – A tensor of shape [time, source_batch, source_dim].</li>
<li><strong>source_contexts</strong> – A tensor of shape [time, source_batch, context_dim].</li>
<li><strong>source_padding</strong> – A tensor of shape [time, source_batch].</li>
<li><strong>source_segment_id</strong> – A tensor of shape [time, source_batch].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">(concated_source_vecs, concated_source_contexts, source_padding,
source_segment_id) tuple where concated_source_vecs is a tensor of shape
[source_seq_len, batch_size * num_heads, orig_source_dim / num_heads],
concated_source_contexts is a tensor of shape [source_batch_size *
num_heads, source_seq_len,  orig_context_dim / num_heads],
source_padding is a tensor of shape [source_seq_len, batch_size *
num_heads] and source_segment_id is a tensor of shape
[source_seq_len, batch_size * num_heads].</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="lingvo.core.attention.MultiHeadedAttention.Params">
<em class="property">classmethod </em><code class="descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MultiHeadedAttention.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MultiHeadedAttention.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for MultiHeadedAttention.</p>
</dd></dl>

<dl class="method">
<dt id="lingvo.core.attention.MultiHeadedAttention.ZeroAttentionState">
<code class="descname">ZeroAttentionState</code><span class="sig-paren">(</span><em>source_seq_length</em>, <em>decoder_batch_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention.html#MultiHeadedAttention.ZeroAttentionState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.attention.MultiHeadedAttention.ZeroAttentionState" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="lingvo.core.attention_test.html" class="btn btn-neutral float-right" title="lingvo.core.attention_test module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="lingvo.core.ops.tokenizer_ops_test.html" class="btn btn-neutral" title="lingvo.core.ops.tokenizer_ops_test module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>